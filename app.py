import streamlit as st
import os
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain.chains import ConversationalRetrievalChain

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
st.set_page_config(page_title="Technical Doc Assistant", layout="wide")
st.title("ü§ñ –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç (NotebookLM Style)")

# 1. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è API (–º–æ–∂–Ω–æ –≤–≤–µ—Å—Ç–∏ –≤ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–µ –∏–ª–∏ —á–µ—Ä–µ–∑ —Å–µ–∫—Ä–µ—Ç—ã)
api_key = st.sidebar.text_input("–í–≤–µ–¥–∏—Ç–µ Gemini API Key:", type="password")
uploaded_file = st.sidebar.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π PDF", type="pdf")

if api_key and uploaded_file:
    os.environ["GOOGLE_API_KEY"] = api_key
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª –≤—Ä–µ–º–µ–Ω–Ω–æ
    with open("temp.pdf", "wb") as f:
        f.write(uploaded_file.get_buffer())

    # 2. –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–ö–µ—à–∏—Ä—É–µ–º, —á—Ç–æ–±—ã –Ω–µ –ø–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞—Ç—å –ø—Ä–∏ –∫–∞–∂–¥–æ–º —Å–æ–æ–±—â–µ–Ω–∏–∏)
    @st.cache_resource
    def prepare_vector_store():
        loader = PyPDFLoader("temp.pdf")
        data = loader.load()
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=300)
        chunks = text_splitter.split_documents(data)
        
        embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
        return Chroma.from_documents(chunks, embeddings)

    vector_store = prepare_vector_store()
    
    # 3. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —á–∞—Ç–∞
    llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash", temperature=0.2)
    qa_chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=vector_store.as_retriever(),
        return_source_documents=True
    )

    # –õ–æ–≥–∏–∫–∞ —á–∞—Ç–∞
    if "messages" not in st.session_state:
        st.session_state.messages = []
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    if prompt := st.chat_input("–ó–∞–¥–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç—É..."):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            result = qa_chain.invoke({"question": prompt, "chat_history": st.session_state.chat_history})
            response = result["answer"]
            st.markdown(response)
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            if result['source_documents']:
                with st.expander("–ü–æ—Å–º–æ—Ç—Ä–µ—Ç—å –∏—Å—Ç–æ—á–Ω–∏–∫–∏"):
                    for doc in result['source_documents'][:2]:
                        st.caption(f"–°—Ç—Ä–∞–Ω–∏—Ü–∞ {doc.metadata['page']}: {doc.page_content[:200]}...")

        st.session_state.messages.append({"role": "assistant", "content": response})
        st.session_state.chat_history.append((prompt, response))
else:
    st.info("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–≤–µ–¥–∏—Ç–µ API –∫–ª—é—á –∏ –∑–∞–≥—Ä—É–∑–∏—Ç–µ PDF –≤ –±–æ–∫–æ–≤–æ–π –ø–∞–Ω–µ–ª–∏.")
